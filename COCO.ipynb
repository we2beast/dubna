{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask-rcnn-coco/frozen_inference_graph.pb\n",
      "[INFO] loading Mask R-CNN from disk...\n",
      "[INFO] could not determine # of frames in video\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f39b7b7bf1cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"detection_out_final\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"detection_masks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# import imutils\n",
    "import time\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "labelsPath = os.path.sep.join([\"mask-rcnn-coco\", \"object_detection_classes_coco.txt\"])\n",
    "LABELS = open(labelsPath).read().strip().split(\"\\n\")\n",
    "#print(LABELS)\n",
    "\n",
    "np.random.seed(42)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "weightsPath = os.path.sep.join([\"mask-rcnn-coco\", \"frozen_inference_graph.pb\"])\n",
    "configPath = os.path.sep.join([\"mask-rcnn-coco\", \"mask_rcnn_inception_v2_coco_2018_01_28.pbtxt\"])\n",
    "print(weightsPath)\n",
    "\n",
    "# load our Mask R-CNN trained on the COCO dataset (90 classes)\n",
    "# from disk\n",
    "print(\"[INFO] loading Mask R-CNN from disk...\")\n",
    "net = cv2.dnn.readNetFromTensorflow(weightsPath, configPath)\n",
    "\n",
    "# initialize the video stream and pointer to output video file\n",
    "vs = cv2.VideoCapture(\"./Bus_11_front_door/2019-6-3_17-12-52.mp4\")\n",
    "writer = None\n",
    "\n",
    "try:\n",
    "    prop = cv2.cv.CV_CAP_PROP_FRAME_COUNT\n",
    "    total = int(vs.get(prop))\n",
    "    print(\"[INFO] {} total frames in video\".format(total))\n",
    "\n",
    "# an error occurred while trying to determine the total\n",
    "# number of frames in the video file\n",
    "except:\n",
    "    print(\"[INFO] could not determine # of frames in video\")\n",
    "    total = -1\n",
    "\n",
    "# loop over frames from the video file stream\n",
    "while True:\n",
    "    # read the next frame from the file\n",
    "    (grabbed, frame) = vs.read()\n",
    "\n",
    "    # if the frame was not grabbed, then we have reached the end\n",
    "    # of the stream\n",
    "    if not grabbed:\n",
    "        break\n",
    "\n",
    "    blob = cv2.dnn.blobFromImage(frame, swapRB=True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    start = time.time()\n",
    "    (boxes, masks) = net.forward([\"detection_out_final\", \"detection_masks\"])\n",
    "\n",
    "    end = time.time()\n",
    "\n",
    "    for i in range(0, boxes.shape[2]):\n",
    "        # extract the class ID of the detection along with the\n",
    "        # confidence (i.e., probability) associated with the\n",
    "        # prediction\n",
    "        classID = int(boxes[0, 0, i, 1])\n",
    "        #print(LABELS[classID])\n",
    "        confidence = boxes[0, 0, i, 2]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            if classID == 0:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the frame and then compute the width and the\n",
    "                # height of the bounding box\n",
    "                (H, W) = frame.shape[:2]\n",
    "                box = boxes[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                boxW = endX - startX\n",
    "                boxH = endY - startY\n",
    "\n",
    "                # extract the pixel-wise segmentation for the object,\n",
    "                # resize the mask such that it's the same dimensions of\n",
    "                # the bounding box, and then finally threshold to create\n",
    "                # a *binary* mask\n",
    "                mask = masks[i, classID]\n",
    "                mask = cv2.resize(mask, (boxW, boxH),\n",
    "                    interpolation=cv2.INTER_NEAREST)\n",
    "                mask = (mask > 0.3)\n",
    "\n",
    "                # extract the ROI of the image but *only* extracted the\n",
    "                # masked region of the ROI\n",
    "                roi = frame[startY:endY, startX:endX][mask]\n",
    "\n",
    "                # grab the color used to visualize this particular class,\n",
    "                # then create a transparent overlay by blending the color\n",
    "                # with the ROI\n",
    "                color = COLORS[classID]\n",
    "                blended = ((0.4 * color) + (0.6 * roi)).astype(\"uint8\")\n",
    "\n",
    "                # store the blended ROI in the original frame\n",
    "                frame[startY:endY, startX:endX][mask] = blended\n",
    "\n",
    "                # draw the bounding box of the instance on the frame\n",
    "                color = [int(c) for c in color]\n",
    "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
    "                    color, 2)\n",
    "\n",
    "                text = \"{}: {:.4f}\".format(LABELS[classID], confidence)\n",
    "                cv2.putText(frame, text, (startX, startY - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    if writer is None:\n",
    "        # initialize our video writer\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter(\"./output/test.avi\", fourcc, 30, (frame.shape[1], frame.shape[0]), True)\n",
    "\n",
    "        if total > 0:\n",
    "            elap = (end - start)\n",
    "            print(\"[INFO] single frame took {:.4f} seconds\".format(elap))\n",
    "            print(\"[INFO] estimated total time to finish: {:.4f}\".format(elap * total))\n",
    "\n",
    "#     cv2.imshow('frame', frame)\n",
    "\n",
    "    writer.write(frame)\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
